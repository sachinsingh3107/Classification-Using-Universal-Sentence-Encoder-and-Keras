# -*- coding: utf-8 -*-
"""classificationUsingTFHUBandKERAS.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13AFGEdCUb8C1TIpDERVo4yrcRlPtVSYT
"""

import tensorflow as tf
import tensorflow_hub as hub
import matplotlib.pyplot as plt
import numpy as np
import os
import pandas as pd
import re
import seaborn as sns
import keras.layers as layers
from keras.models import Model
from keras import backend as K
np.random.seed(10)

module_url = "https://tfhub.dev/google/universal-sentence-encoder-large/3" #@param ["https://tfhub.dev/google/universal-sentence-encoder/2", "https://tfhub.dev/google/universal-sentence-encoder-large/3"]

embed = hub.Module(module_url)

embed_size = embed.get_output_info_dict()['default'].get_shape()[1].value
embed_size

!git clone https://github.com/sachinsingh3107/textfilesfortfhub/

option_list = ['review_id','product_id','agent_name','sound','noise_cancel','comfort','customer_care','microphone','connectivity','price','built','bluetooth','bass','review_text']

def get_dataframe(filename, option):
    lines = open(filename, 'r').read().splitlines()
    data = []
    
    index = option_list.index(option)
    for i in range(1, len(lines)):
        label = lines[i].split(',')[index]
        label = label.split(",")[0]
        text = ' '.join(lines[i].split(',')[13])
        text = re.sub('[^A-Za-z0-9 ,\?\'\"-._\+\!/\`@=;:]+', ' ', text)
        data.append([label, text])

    df = pd.DataFrame(data, columns=['label', 'text'])
    df.label = df.label.astype('category')
    return df

# df_train = get_dataframe('textfilesfortfhub/train_5500.txt')
# df_train.head()

option = 'noise_cancel'#@param ["review_id","product_id","agent_name","sound","noise_cancel","comfort","customer_care","microphone","connectivity","price","built","bluetooth","bass"]

train_data = get_dataframe('textfilesfortfhub/train_5500.csv', option)

df_train = pd.read_csv('textfilesfortfhub/train_5500.csv')
  
df_train = df_train[[option,'review_text']]
df_train.head()  


#df_train.label = df_train.label.astype('sound')

#df_train = get_dataframe('txtfiles/test_data.txt')

category_counts = len(train_data.label.cat.categories)
category_counts

#Explicitly cast the input as a string

def UniversalEmbedding(x):
  return embed(tf.squeeze(tf.cast(x, tf.string)), signature="default", as_dict=True)["default"]

input_text = layers.Input(shape=(1,), dtype="string")
embedding = layers.Lambda(UniversalEmbedding, output_shape=(512,))(input_text)
dense = layers.Dense(1024, activation='relu')(embedding)
dense = layers.Dense(512, activation='relu')(dense)
dense = layers.Dense(256, activation='relu')(dense)
dropout = layers.Dropout(rate = 0.3)
dense = layers.Dense(100, activation='relu')(dense)
#bnorm = layers.BatchNormalization()(dense)
pred = layers.Dense(category_counts, activation='softmax')(dense)
model = Model(inputs=[input_text], outputs=pred)
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
model.summary()

train_text = df_train['review_text'].tolist()
train_text = np.array(train_text, dtype=object)[:, np.newaxis]

train_label = np.asarray(pd.get_dummies(train_data.label), dtype = np.int8)

# train_label[:4]

test_data = get_dataframe('textfilesfortfhub/test_5500.csv', option)

df_test = pd.read_csv('textfilesfortfhub/test_5500.csv')
  
df_test = df_test[[option,'review_text']]
df_test.head()

test_text = df_test['review_text'].tolist()
test_text = np.array(test_text, dtype=object)[:, np.newaxis]
test_label = np.asarray(pd.get_dummies(test_data.label), dtype = np.int8)

# #Train Keras model and save weights
# with tf.Session() as session:
#   K.set_session(session)
#   session.run(tf.global_variables_initializer())
#   session.run(tf.tables_initializer())
#   history = model.fit(train_text, 
#             train_label,
#             validation_data=(test_text, test_label),
#             epochs=100,
#             batch_size=32,
#             class_weight = 'balanced')
  
  
#   model_yaml = model.to_yaml()
#   with open("model_{}.yaml".format(option), "w") as yaml_file:
#     yaml_file.write(model_yaml)
# # serialize weights to HDF5
#   model.save_weights("model_{}.h5".format(option))
#   print("Saved model_{} to disk".format(option))


sess = tf.Session()
K.set_session(sess)
sess.run(tf.global_variables_initializer())
sess.run(tf.tables_initializer())

history = model.fit(train_text, 
          train_label,
          validation_data=(test_text, test_label),
          epochs=20,
          batch_size=32,
          class_weight = 'balanced')

model_yaml = model.to_yaml()
with open("model_{}.yaml".format(option), "w") as yaml_file:
  yaml_file.write(model_yaml)
# serialize weights to HDF5
model.save_weights("model_{}.h5".format(option))
print("Saved model_{} to disk".format(option))

!ls -alh | grep model_noise_cancel.h5

#Make predictions
new_text = ["Headphone sounds great i definitely recommend this product, but the noise cancellation doesn't work well.  I spoke to the customer service folks and they were no good", "i don't recommend to buy,. the customer care was not good"]
new_text = np.array(new_text, dtype=object)[:, np.newaxis]

outcome = {}

with tf.Session() as session:
  K.set_session(session)
  session.run(tf.global_variables_initializer())
  session.run(tf.tables_initializer())

  
  model.load_weights('./model_{}.h5'.format(option))  
  predicts = model.predict(new_text, batch_size=32)

categories = train_data.label.cat.categories.tolist()
#print(categories)
predict_logits = predicts.argmax(axis=1)
predict_labels = [categories[logit] for logit in predict_logits]
predict_labels



